{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b87ae1",
   "metadata": {},
   "source": [
    "# Python fundamentals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8232786f",
   "metadata": {},
   "source": [
    "# Basic Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce3e714",
   "metadata": {},
   "source": [
    "## String Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb9219f",
   "metadata": {},
   "source": [
    "string='str'\n",
    "len(str)\n",
    "slicing: str[:] str[0] str[start,stop,step]\n",
    "str.lower()\n",
    "str.upper()\n",
    "str.capitalize()\n",
    "str.startwith()\n",
    "str.endwith()  judge\n",
    "str.strip() trim\n",
    "str.join(list)  list to str\n",
    "string.split(',') str to list\n",
    "str.replace('a', 'b') \n",
    "str1 + str2 + ' ' \n",
    "\n",
    "name='bob'\n",
    "fruit='banana'\n",
    "f'Hi, my name is {name} and I like {fruit}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b91b9840",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = ['p', 's', 'c', 'x', 'd']\n",
    "var.remove('d') ## is nontype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cc99e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p', 's', 'c', 'x']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e900bf6",
   "metadata": {},
   "source": [
    "## Number Caculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d62bb4",
   "metadata": {},
   "source": [
    "a%b a remainder of b\n",
    "a**b a power of b\n",
    "a//b floor division\n",
    "a!=b a>=b a<=b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f91e7e",
   "metadata": {},
   "source": [
    "## List,Dict, Turplet, Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5fccab",
   "metadata": {},
   "source": [
    "list = ['a', 'b', 'c', 'd', 'e'] or   list(a) conver to list\n",
    "turple= ('a', 'b', 'c', 'd', 'e')    turple(a)\n",
    "dict = {'a':1, 'b':2, 'c':3}       dict(a)\n",
    "set = {1, 2, 3, 4}        set(a)     only one input in these brackets\n",
    "\n",
    "List operations list and set and dict are mutable\n",
    "slicing: list[:] list[0] list[start,stop,step]\n",
    "list.append()\n",
    "del(list[index])\n",
    "list.pop()  remove the end item of a list, returns the removed element\n",
    "list.remove(item)\n",
    "list.sort()\n",
    "list1+list2 = list3   list1 + ['a'] = list2\n",
    "\n",
    "turple is similar to list but not mutable\n",
    "\n",
    "\n",
    "set is similar to list but no order\n",
    "set1.intersection(set2)\n",
    "set1.union(set2)\n",
    "set1.difference(my_set2)\n",
    "\n",
    "Dictionary operations\n",
    "dict['key']    to value no order\n",
    "dict.pop('key',None) delete a key\n",
    "dict.keys() call all keys in a list\n",
    "dict.dict_values() call all values in a list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac092081",
   "metadata": {},
   "source": [
    "## Loop, Function, Try Enumerate Zip Datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b986cedb",
   "metadata": {},
   "source": [
    "x=input()\n",
    "try: except:\n",
    "if: else:\n",
    "for str in list:\n",
    "\n",
    "[x+1 for x in list1]\n",
    "\n",
    "while n>1:\n",
    "\n",
    "enumerate(list)\n",
    "for i, x in enumerate(list):\n",
    "i is the index\n",
    "\n",
    "\n",
    "zip(list1,list2)\n",
    "\n",
    "dict1 = dict(zip(a,b))\n",
    "\n",
    "import datetime\n",
    "datetime.datetime.now()\n",
    "dt = datetime.datetime.strptime('30/01/2020', '%d/%m/%Y')\n",
    "dt = datetime.datetime.strptime(str, '%d/%m/%Y')\n",
    "dt.year\n",
    "dt.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfe446f",
   "metadata": {},
   "source": [
    "## Reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d3990e",
   "metadata": {},
   "source": [
    "import re\n",
    "\\w word and number\n",
    "\\d number\n",
    ". anything\n",
    "* 0 or more\n",
    "+  1 or more\n",
    "[a-z] or [^a-z]  \n",
    "a{1,3} a repeat 1 to 3times\n",
    "ab?c may include b or not\n",
    "\\? plaine question mark\n",
    "\\s space \n",
    "\\t tab\n",
    "\\n new line\n",
    "\\r return\n",
    "^ $ start and end of a line\n",
    "() capture information in the bracket\n",
    "(a(b)c) () () multiple captures\n",
    "([cb]ats*|[dh]ogs?) differentiate sets\n",
    "\n",
    "\n",
    "\n",
    "re.findall('\\n(\\w\\s\\w+), str) return a list of captured strs\n",
    "pnumber=re.find('^(\\d{3})-(\\d{3})-(\\d{4})$',phone_number) return a turple\n",
    "pnumber.group(1)  pnumber.group(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395b7793",
   "metadata": {},
   "source": [
    "## Load file in Python jason"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310bcb7e",
   "metadata": {},
   "source": [
    "f = open(\"mnist_train_100.csv\", 'r')  \n",
    "mnist = f.readlines()\n",
    "\n",
    "with open('data/roders/csv', 'r') as file:\n",
    "   data = file.readlines()\n",
    "   \n",
    " data is a list\n",
    " data[i] is each row of csv \n",
    " \n",
    "import jason\n",
    " \n",
    "with open('data/user.json', encoding='utf-8') as file:\n",
    "    user = json.load(file)         user is a dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e4434f",
   "metadata": {},
   "source": [
    "## web scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9fe1f7",
   "metadata": {},
   "source": [
    "from bs4 import BeautifulSoup # For HTML parsing\n",
    "import requests # Website connectio ns\n",
    "from time import sleep # To prevent overwhelming the server between connections\n",
    "from collections import Counter # Keep track of our term counts\n",
    "import pandas as pd # For converting results to a dataframe and bar chart plots\n",
    "import json # For parsing json\n",
    "!pip install selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "\n",
    "url = 'https://adfadsfsdfadsf'\n",
    "driver.get(\"url\")\n",
    "result = requests.get(url) or driver.page_source\n",
    "soup = BeautifulSoup(reuslt.content or driver.page_source) soup is a huge string\n",
    "\n",
    "soup.find('span', class_='companyName').a.text)\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "search_bar=driver.find_element_by_css_selector('#SearchKeyword')\n",
    "search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "import time\n",
    "for x in range(3):\n",
    "    time.sleep(10)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f55793f",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d9b050",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a8a3fcb",
   "metadata": {},
   "source": [
    "## dataframe basics construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c665c69",
   "metadata": {},
   "source": [
    "data = [[3, 7],\n",
    "        [1, 9],\n",
    "        [5, 3]]\n",
    "\n",
    "df = pd.DataFrame(data, columns=['foo', 'bar'], index=['a', 'b', 'c'])\n",
    "\n",
    "data = [(3, 7),\n",
    "        (1, 9),\n",
    "        (5, 3)]\n",
    "\n",
    "df = pd.DataFrame(data, columns=['foo', 'bar'], index=['a', 'b', 'c'])\n",
    "\n",
    "data = {'foo': [3, 1, 5],\n",
    "        'bar': [7, 9, 3]}\n",
    "\n",
    "df = pd.DataFrame(data, index=['a', 'b', 'c'])\n",
    "\n",
    "data = [{'foo': 3, 'bar': 7},\n",
    "        {'bar': 1, 'foo': 9},\n",
    "        {'foo': 5, 'bar': 3}]\n",
    "\n",
    "df = pd.DataFrame(data, index=['a', 'b', 'c'])\n",
    "\n",
    "\n",
    "df= pd.DataFrame(x,index='',columns='')\n",
    "df= pd.DataFrame(x,index=pd.date_range(start='2013-01-01', end='2013-01-06'),columns='')\n",
    "df = pd.read_csv('', sep='', names=['columns'],delim_whitespace='')\n",
    "df.info()\n",
    "df.head()\n",
    "df.tail()\n",
    "df.shape\n",
    "df.index\n",
    "df.columns\n",
    "df.describe()\n",
    "df.loc[[0]]\n",
    "df.loc[[1,2,3],['columns','column']]\n",
    "pd.get_dummies(df)\n",
    "df.sort_values(by='column',ascending=True)\n",
    "df.sort_index(by='column',ascending=True)\n",
    "df['column'][df['column'] > 90]\n",
    "df.reset_index()\n",
    "df.rename(index='',columns='')\n",
    "df.set_index(['columns1','column2'])\n",
    "df.plot\n",
    "df.fillna()\n",
    "df.drop(columns='')\n",
    "df.pivot_table('column1', index='column2', columns='column3')\n",
    "\n",
    "# df sampling\n",
    "df_frac = df.sample(frac=0.7)\n",
    "df_rest = df.loc[~df.index.isin(df_frac.index)]\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(centers=[[1, 1], [2, 2]], random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d4c54c",
   "metadata": {},
   "source": [
    "## dataseries basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47feafb5",
   "metadata": {},
   "source": [
    "ds = pd.Series(list)\n",
    "ds = pd.Series(dict)\n",
    "ds = df['column']\n",
    "\n",
    "ds.reindex(list)\n",
    "ds.isnull()\n",
    "ds.sum()\n",
    "ds.cusum()\n",
    "ds.mean()\n",
    "ds.value_counts(normalize=True)\n",
    "pd.to_datetime(ds) ->ds.dt.year ds.dt.month ds.dt.dayofweek ds.dt.hour\n",
    "ds.keys()\n",
    "ds.values()\n",
    "\n",
    "ds.count() - Number of non-null observations\n",
    "sum() - Sum of values\n",
    "mean() - Mean of values\n",
    "median() - Arithmetic median of values\n",
    "min() - Minimum\n",
    "max() - Maximum\n",
    "std() - Bessel-corrected sample standard deviation\n",
    "var() - Unbiased variance\n",
    "skew() - Sample skewness (3rd moment)\n",
    "kurt() - Sample kurtosis (4th moment)\n",
    "quantile() - Sample quantile (value at %)\n",
    "apply() - Generic apply\n",
    "cov() - Unbiased covariance (binary)\n",
    "corr() - Correlation (binary)\n",
    "\n",
    "ds.map(('STREET':'ST'))\n",
    "ds.plot()\n",
    "ds.str.find() ....\n",
    "\n",
    "import re\n",
    "ds.str.replace(r'\\s(STREET)\\d++','ST')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa68e265",
   "metadata": {},
   "source": [
    "## DF merge aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf791a71",
   "metadata": {},
   "source": [
    "## appending\n",
    "pd.concat([df1,df2])\n",
    "pd.concat([df3, df4], axis=1)\n",
    "\n",
    "pd.merge(df1,df2,left_on='column1',right_on='column2',how='left',suffixes=['_a','_b'])\n",
    "df.merge(df1)\n",
    "\n",
    "pd.join(df1,df2)\n",
    "df1.join(df2)  ## merge on index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42685f6",
   "metadata": {},
   "source": [
    "## aggregation function\n",
    "\n",
    "df.groupby('column')\n",
    "df.groupby('column1')['column2'].describe()\n",
    "\n",
    "Aggregation\tDescription\n",
    "df.groupby('column')['column2'].count()\tTotal number of items\n",
    "df.groupby('column')['column2'].first(), last()\tFirst and last item\n",
    "df.groupby('column')['column2'].mean(), median()\tMean and median\n",
    "df.groupby('column')['column2'].min(), max()\tMinimum and maximum\n",
    "df.groupby('column')['column2'].std(), var()\tStandard deviation and variance\n",
    "df.groupby('column')['column2'].mad()\tMean absolute deviation\n",
    "df.groupby('column')['column2'].prod()\tProduct of all items\n",
    "df.groupby('column')['column2'].sum()\tSum of all items\n",
    "\n",
    "df.groupby('column')['column2'].aggregate(),\n",
    "df.groupby('column')['column2'].filter(),  \n",
    "df.groupby('column')['column2'].transform(),\n",
    "df.groupby('column')['column2'].apply().\n",
    "\n",
    "df.groupby('column').filter(filter_func)\n",
    "def filter_func(x):\n",
    "    return x['column1'].std() > 4\n",
    "\n",
    "df.groupby('column').agg({'column2': 'min', 'column3': ['max', 'mean', 'sum']})\n",
    "\n",
    "\n",
    "df.groupby('column').transform(lambda x: x / x.mean())\n",
    "\n",
    "df.groupby('key').apply(norm_by_data2)\n",
    "def norm_by_data2(x):\n",
    "    # x is a DataFrame of group values\n",
    "    x['data1'] /= x['data2'].sum()\n",
    "    return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae7f4ab",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "### plt plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list1,list2,'xr',label='test1',alpha=0.4,markersize=4); ## x shape red alpha: transparentcy\n",
    "plt.plot([0, 1], [0, 2], '-',linewidth=3,label='t');  ## trendline\n",
    "plt.title('Scatter Plot')\n",
    "plt.legend(loc='lower right) \n",
    "plt.xlabel('base')\n",
    "plt.ylabel('yield')\n",
    "plt.grid(alpha=0.2);\n",
    "\n",
    "plt.scatter(x, y, marker='x', c=['g'], alpha=0.25, label='Cats')\n",
    "\n",
    "\n",
    "\n",
    "plt.subplots(2, 2) or\n",
    "fig, axs = plt.subplots(2,2, figsize=(10,10));\n",
    "axs[0,0].plot(x1,y1, 'x');\n",
    "axs[0,0].set_xlabel('x');\n",
    "axs[0,0].set_ylabel('y');\n",
    "axs[0,0].set_title('First Plot');\n",
    "\n",
    "axs[0,1].plot(x2,y2, 'xg');\n",
    "axs[0,1].set_xlabel('x');\n",
    "axs[0,1].set_ylabel('y');\n",
    "axs[0,1].set_title('Second Plot');\n",
    "\n",
    "axs[1,0].plot(x3,y3, 'xr');\n",
    "axs[1,0].set_xlabel('x');\n",
    "axs[1,0].set_ylabel('y');\n",
    "axs[1,0].set_title('Third Plot');\n",
    "\n",
    "axs[1,1].plot(x4,y4, 'xc');\n",
    "axs[1,1].set_xlabel('x');\n",
    "axs[1,1].set_ylabel('y');\n",
    "axs[1,1].set_title('Fourth Plot');\n",
    "\n",
    "fig.tight_layout(pad=1.5);\n",
    "\n",
    "plt.hist(df['column'], bins=12)\n",
    "\n",
    "plt.boxplot(df['column'])\n",
    "plt.title('AB')\n",
    "plt.xticks([1,2],'column')\n",
    "plt.ylabel('Y');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9f43c7",
   "metadata": {},
   "source": [
    "### pandas plot\n",
    "df['column'].value_counts().plot(kind='bar',label='Robbery'); or 'line' 'scatter'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb679d5",
   "metadata": {},
   "source": [
    "### seaborn plot\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "sns.histplot(data=df, x=df['column'], kde=True)\n",
    "\n",
    "sns.pairplot(df) ## 2-D plot from each random column pairs\n",
    "sns.pairplot(df, hue='category_column'); ## pairplot labeled by categories\n",
    "sns.heatmap(df.corr(), annot=True); heat plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd9de82",
   "metadata": {},
   "source": [
    "### ploty\n",
    "\n",
    "!pip install plotly\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go \n",
    "\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "#### bar plot\n",
    "crime_types = crime['MCI'].value_counts()\n",
    "data = go.Bar(x=crime_types.index,\n",
    "             y = crime_types.values)\n",
    "go.Figure(data)\n",
    "\n",
    "#### layout\n",
    "layout = go.Layout(title='Premise Type Counts',\n",
    "                  xaxis={'title': 'Premise'},\n",
    "                  yaxis={'title': 'Counts'})\n",
    "\n",
    "go.Figure(data, layout)\n",
    "\n",
    "#### scatter\n",
    "data = go.Scatter(\n",
    "          x=crime_date['occurrencedate'],\n",
    "          y=crime_date['count'])\n",
    "\n",
    "layout = go.Layout(  \n",
    "    title='Crime Types Count')\n",
    "\n",
    "go.Figure([data],layout=layout)\n",
    "\n",
    "\n",
    "#### box plot\n",
    "data = go.Box(x=iris['target'],\n",
    "    y=iris['sepal length (cm)'],\n",
    "    boxpoints='all',\n",
    "    #name='flowers'\n",
    ")\n",
    "\n",
    "Layout = go.Layout(\n",
    "    width = 500,\n",
    "    yaxis = {'title':'sepal length(cm)'}\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[data],layout=Layout)\n",
    "py.iplot(fig)\n",
    "\n",
    "#### multiple plots from different dfs\n",
    "robbery = go.Scatter(x=robbery_date['occurrencedate'],\n",
    "                    y=robbery_date['count'],\n",
    "                    name='Robbery')\n",
    "\n",
    "auto = go.Scatter(x=auto_date['occurrencedate'],\n",
    "                  y=auto_date['count'],\n",
    "                 name = 'Auto Theft')\n",
    "\n",
    "layout = go.Layout(title='MCI Trends', \n",
    "                  xaxis = {'title': 'Occurrence Year'},\n",
    "                  yaxis = {'title': 'Count'})\n",
    "\n",
    "go.Figure([robbery, auto], layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380abd6e",
   "metadata": {},
   "source": [
    "### Feature Engineering \n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "#### Threshold Capping \n",
    "df['column'].plot(kind='box')\n",
    "def TCap(columns):\n",
    "        q1, q3 = X_train[i].quantile([0.25, .75])\n",
    "\n",
    "        iqr = q3 - q1\n",
    "\n",
    "        top_boundary = q3 + (iqr * 1.5)\n",
    "\n",
    "        X_train.loc[X_train['column'] >= top_boundary,['column']] = top_boundary\n",
    "        \n",
    "\n",
    "### Binning\n",
    "bin_boundaries = [0, 18, 35, 200]\n",
    "bin_labels = ['child', 'young_adult', 'adult']\n",
    "\n",
    "df['age_binned'] = pd.cut(df['age'], bins=bin_boundaries, labels=bin_labels)\n",
    "\n",
    "or\n",
    "\n",
    "q_labels = ['q1', 'q2', 'q3', 'q4']\n",
    "\n",
    "pd.qcut(df['fare'], 4, labels=q_labels).value_counts()\n",
    "\n",
    "\n",
    "### Category to numbers\n",
    "#### Create a new column with the 'title' categories encoded as their frequency counts\n",
    "df['title_freq_encoded'] = df.groupby('title')['title'].transform('count')\n",
    "df['title_target_encoded'] = df.groupby('title')['survived'].transform('mean')\n",
    "\n",
    "#### One way to do frequency encoding\n",
    "df['title'].replace(df['title'].value_counts()).head()\n",
    "\n",
    "###  Interaction (sum, difference, product, ratio)\n",
    "- family_size = sibsp + parch\n",
    "- account_age = loan_date - account_create_date\n",
    "- revenue = price * quantity, area = w * h\n",
    "- profit/sales, height/weight = BMI, income/population, price/area\n",
    "- sex * pclass = (m, pclass1), (f, class1), ... ,(f, pclass3)\n",
    "\n",
    "### Datetime\n",
    "- Periodicity (dayofweek, month, year, q1-q4, season, time of day)\n",
    "- Time difference (age = current_date - dob, end_date - start_date = length of contract)\n",
    "- time since/time until (Holiday - current_date)\n",
    "- Time Interval (number of calls in last 1m, 3m, 6m, 1yr)\n",
    "\n",
    "### Coordinates (Latitude, Longitude)\n",
    "- (geopandas*)\n",
    "- Group by Neighbourhood (aggregate on neighbourhood)\n",
    "- Distance to exepensive house\n",
    "- Distance to (shops, transportation, hospital, schools)\n",
    "\n",
    "### Price\n",
    "- Group by categories, aggregate price\n",
    "- Binning (0-100: cheap, 100-1000: medium...)\n",
    "- Decimal value (14.99 -> 99)\n",
    "    - indicator sale price\n",
    "    - Click interval => 2.00 vs 2.225"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd837b52",
   "metadata": {},
   "source": [
    "# Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca480db",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "array1 = np.array(range(1000000))\n",
    "array2 = np.array(range(1000000))\n",
    "array = np.array(list)\n",
    "array.mean()\n",
    "array[:] array[0] array[start,stop,step]\n",
    "\n",
    "array = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
    "array>6\n",
    "array[array>6]\n",
    "array[:2,2:]\n",
    "array[1,0] array[0,3]\n",
    "\n",
    "array.shape\n",
    "array.reshape((2,3))\n",
    "list = [\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3],\n",
    "]\n",
    "np.array(list)\n",
    "np.sqrt(ds)\n",
    "np.std(ds)\n",
    "\n",
    "\n",
    "#### create matrice or arrays\n",
    "np.zeros((3, 3, 6)) 3dfs, each df 3 rows 6 columns of 0s\n",
    "\n",
    "np.ones((3, 3, 6)) 3dfs, each df 3 rows 6 columns of 1s\n",
    "\n",
    "np.random.random((3, 3, 6)) 3dfs, each df 3 rows 6 columns of random floats(0.1)\n",
    "\n",
    "np.random.randn((3, 3, 6)) 3dfs, each df 3 rows 6 columns of random floats normal distribution with mean of 0\n",
    "\n",
    "np.empty((3, 3, 6)) 3dfs, each df 3 rows 6 columns of space\n",
    "\n",
    "np.full((3, 3, 6),7) 3dfs, each df 3 rows 6 columns of 7s\n",
    "\n",
    "np.arange(10,125,5) array from 10 to 125 step 5\n",
    "\n",
    "\n",
    "np.linspace(0,2,9) array from 0 to 2 of 9 values evenly spaced\n",
    "\n",
    "np.eye(4) create identity matrix\n",
    "\n",
    "#### convert image to digits\n",
    "f = open(\"mnist_train_100.csv\", 'r')  # In this tutorial, the mnist sample is stored in csv format\n",
    "mnist = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = plt.figure(figsize=(10,10))\n",
    "count=0\n",
    "for line in mnist[:9]:\n",
    "    count += 1\n",
    "    linebits = line.split(',')\n",
    "    imarray = np.asfarray(linebits[1:]).reshape((28,28))   ## reshape the list to 28X28\n",
    "    plt.subplot(3,3,count)\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    plt.title(\"Label is \" + linebits[0])\n",
    "    plt.imshow(imarray, cmap='Greys', interpolation='None')\n",
    "    \n",
    "#### sampling\n",
    "np.random.shuffle(array)\n",
    "\n",
    "A random permutation, to split the data randomly by randomly selecting index\n",
    "np.random.seed(0)\n",
    "indices = np.random.permutation(len(iris_X))\n",
    "data = np.random.randint(0, 10000, 5000) ## sampling\n",
    "sample = random.sample(list(data), 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeb4a5c",
   "metadata": {},
   "source": [
    "#### spicy statisitic\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "#### probability function\n",
    "x = np.random.randn(5000)\n",
    "x.sort()\n",
    "mu = np.mean(x)\n",
    "sigma = np.std(x)\n",
    "print(mu, sigma)\n",
    "plt.plot(x, sp.stats.norm.pdf(x,mu,sigma), color='teal')\n",
    "plt.title('Probability Density Function')\n",
    "plt.show()\n",
    "\n",
    "sp.stats.binom.cdf(k=8,        # Probability of k = 8 successes or less\n",
    "                   n=10,       # With 10 flips\n",
    "                   p=0.8)      # And success probability 0.8\n",
    "                   \n",
    "                   \n",
    "sp.stats.binom.pmf(k=8,        # Probability of k = 8 successes\n",
    "                   n=10,       # With 10 flips\n",
    "                   p=0.8)      # And success probability 0.8\n",
    "\n",
    "biased_coin_flips = sp.stats.binom.rvs(n=10,        # Number of flips per trial\n",
    "                                       p=0.8,       # Success probability\n",
    "                                       size=1000)  # Number of trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35513f30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf9e1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a722737",
   "metadata": {},
   "source": [
    "# Webscrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c2dbae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae86ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0301a73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd9fb67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee063243",
   "metadata": {},
   "source": [
    "# Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b2645b",
   "metadata": {},
   "source": [
    "## split data into train/test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82be87b",
   "metadata": {},
   "source": [
    "## KNN analysis\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "### load data\n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "X = dataset['data']\n",
    "y = dataset['target']\n",
    "feature_names = dataset['feature_names']\n",
    "\n",
    "### split data into train/test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calc_distance(a,b):  # calculate the distance between two points\n",
    "    sum=0\n",
    "    for i,x in zip(a,b):\n",
    "        sum+=(i-x)**2\n",
    "    result=math.sqrt(sum)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def calc_dist_to_neighbours(g):\n",
    "    list1=[]\n",
    "    for i in range(X_train.shape[0]):\n",
    "        if i!=g:\n",
    "            list1.append(tuple([i,calc_distance(X_train[g],X_train[i])]))\n",
    "        else:\n",
    "            continue\n",
    "    return list1\n",
    "\n",
    "\n",
    "\n",
    "def find_knn(a): # find 5 closest data of the selected data\n",
    "    x0=calc_dist_to_neighbours(a)\n",
    "    a0=sorted(x0,key=lambda x:x[1])[:5]\n",
    "    return a0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calc_knn_pred(x): # predict the outcome of the selected data based on the 5 closest data\n",
    "    a=find_knn(x)\n",
    "    indeces=[]\n",
    "    result=[]\n",
    "    for n in a:\n",
    "        indeces.append(n[0])\n",
    "        result.append(y_train[n[0]])\n",
    "    predicted_probability=np.mean(np.array(result))*100\n",
    "    prediction=round(predicted_probability/100)\n",
    "    c=[predicted_probability,prediction]\n",
    "    return c\n",
    "\n",
    "prediction_set=[]  # collect the predictions for each data set\n",
    "for i in range(X_train.shape[0]):\n",
    "    prediction=calc_knn_pred(i)\n",
    "    prediction_set.append(prediction)\n",
    "prediction_set\n",
    "\n",
    "compare=[]       # compare the predictions with corresponding facts\n",
    "for m,n in zip(prediction_set,y_train):\n",
    "    if m[1]==n:\n",
    "        compare.append(1)\n",
    "    else:\n",
    "        compare.append(0)\n",
    "accuracy=np.mean(np.array(compare)) # get the accuracy of prediction\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91523b0a",
   "metadata": {},
   "source": [
    "### Evaluation matrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('MSE:', mean_squared_error(y_test, y_pred))\n",
    "print('r2:', r2_score(y_test, y_pred)\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "confusion_matrix(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c0ebc1",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "from imblearn.pipeline import make_pipeline,Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "#### imblance data\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "#### Bag of word vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_features=10000)\n",
    "\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "wordcloud = WordCloud().generate('text')\n",
    "\n",
    "plt.imshow(wordcloud);\n",
    "#### dimension reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca= PCA() #PCA(0.9)\n",
    "\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f5d7aa",
   "metadata": {},
   "source": [
    "### algorithms\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "model.coef_\n",
    "\n",
    "model.intercept_\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "pd.Series(model.feature_importances_).plot(kind='bar');\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(random_state=1)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier()\n",
    "pd.Series(model.feature_importances_, index=X.columns).sort_values().plot(kind='bar');\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model =  RandomForestClassifier(n_estimators=500, max_depth=10, n_jobs=-1, random_state=42)\n",
    "\n",
    "#### Unsupervised KMeans\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from scikitplot.cluster import plot_elbow_curve\n",
    "from scikitplot.metrics import plot_silhouette\n",
    "\n",
    "model = KMeans()\n",
    "plot_elbow_curve(model, df, cluster_ranges=range(1, 11), figsize=(10, 7)); ### elbow plot to find best cluster numbers\n",
    "\n",
    "model = KMeans(2, random_state=1)\n",
    "model.fit(diabetes)\n",
    "plot_silhouette(diabetes, model.labels_);  #silhoutte plot to find the best cluster numbers\n",
    "\n",
    "\n",
    "model = KMeans(n_clusters=3, random_state=1) ### convert to 3 labels\n",
    "model.fit(df)\n",
    "\n",
    "df['clusters'] = model.labels\n",
    "\n",
    "\n",
    "#### Deep Learning Neuron Network\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dense, Dropout, Activation\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=[28, 28]))\n",
    "##convolution model.add(Conv2D(32, (3, 3), input_shape=(28, 28, 1), activation='relu'))\n",
    "##convolution model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dense(200, activation='relu', activity_regularizer=regularizers.l1(0.1)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.1), metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_split=0.1, batch_size=128)\n",
    "pd.DataFrame(history.history).plot();\n",
    "model.evaluate(X_test, y_test)\n",
    "\n",
    "- Images (CNN)\n",
    "    - handwriting recognition (atm) (OCR-object character recognition)\n",
    "    - facial recognition (security)\n",
    "    - Medical image (MRI)\n",
    "    - military stuff (drones, missiles)\n",
    "    - self driving car\n",
    "    - image reconstruction\n",
    "    - style transfer\n",
    "    - Deep fake\n",
    "    - image generation\n",
    "    - human movement improvement\n",
    "    - Satellite images (farming, housing, overfishing, stock price - parking lot)\n",
    "    - Gait prediction (minority report)\n",
    "- Text (word2vec, rnn/lstm)\n",
    "    - sentiment analysis\n",
    "    - news articles to predict stock price\n",
    "    - translation\n",
    "    - chatbots\n",
    "    - text classification\n",
    "    - law (paralegals)\n",
    "    - Spam decection\n",
    "- Other stuff\n",
    "    - Timeseries (rnn/lstm)\n",
    "    - Recommenders (collaborative filtering)\n",
    "    - games (alphaGo, alphazero) (deep reinforcement learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157420dd",
   "metadata": {},
   "source": [
    "### decision tree Gini Impurity\n",
    "\n",
    "$$G = \\sum_i^C p(i) * (1 - p(i))$$\n",
    "\n",
    "https://victorzhou.com/blog/gini-impurity/\n",
    "\n",
    "((50/100) * (1 - (50/100))) +  ((50/100) * (1 - (50/100)))\n",
    "\n",
    "### Using the formula, calculate the gini for the left node\n",
    "\n",
    "((38/46) * (1 - (38/46))) +  ((8/46) * (1 - (8/46)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b1d809",
   "metadata": {},
   "source": [
    "## Ensemble\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "### boosting\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "### Stacking\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "estimators = [\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('knn2', KNeighborsClassifier(n_neighbors=7)),\n",
    "    ('dt', DecisionTreeClassifier()),\n",
    "    ('dt2', DecisionTreeClassifier(max_depth=20)),\n",
    "    ('lr', LogisticRegression()),\n",
    "    ('lr2', LogisticRegression(C=0.1)),\n",
    "    xgboost\n",
    "    rf\n",
    "    catboost\n",
    "]\n",
    "\n",
    "clf = StackingClassifier(\n",
    "    estimators=estimators, final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "cross_val_score(clf, X, y, cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe2bbab",
   "metadata": {},
   "source": [
    "### Output & VALIDATION\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X_train_full, y_train_full, cv=5, scoring='f1')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#### use GridSearch to tune the hyperparameters\n",
    "pipe = Pipeline([\n",
    "      ##('Smote',SMOTE()),\n",
    "        ##('o', ros), \n",
    "        ##('u', rus),\n",
    "     ('scaling', MinMaxScaler()),\n",
    "     ##('scaling', RobustScaler()),\n",
    "     ##('scaling', StandardScaler()),\n",
    "    ('simpleimputer', SimpleImputer()),\n",
    "    ##('DT', DecisionTreeClassifier())\n",
    "    ##('LR', LogisticRegression())\n",
    "    ('DTR',DecisionTreeRegressor())\n",
    "    ##('RF', RandomForestClassifier)\n",
    "        \n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    ##'o__sampling_strategy':[1.0, 0.9, 0.7],\n",
    "    ##'RF__max_features':  [1 to 10],\n",
    "    ##'RF__n_estimators':  [10, 100, 1000],\n",
    "    ##'RF__max_features':  ['sqrt', 'log2'], \n",
    "    ##'LR__penalty':  ['l2'],\n",
    "    ##'LR__C':  [25.0, 10.0,5.0, 1.0, 0.5, 0.1],\n",
    "   #### 'LR__solver': ['newton-cg', 'lbfgs'],\n",
    "    'DTR__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'], \n",
    "    'DTR__max_features':  [ 'auto', 'sqrt', 'log2'],\n",
    "    'DTR__max_depth': [10, 50, 100, 150, 200],\n",
    "    'DTR__splitter' : ['best', 'random']\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "gs = GridSearchCV(pipe, parameters, cv=10, scoring='f1', n_jobs=2, verbose=2)\n",
    "\n",
    "gs.fit(X_input_view, y_input_view)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2fa7cb",
   "metadata": {},
   "source": [
    "## Feature Analysis\n",
    "\n",
    "### DT explanation on other models\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nn = MLPClassifier((50, 50), random_state=1)\n",
    "\n",
    "nn.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_proba = nn.predict_proba(X_train)[:, 1]\n",
    "\n",
    "y_proba[:5]  \n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor()\n",
    "\n",
    "dt.fit(X_train, y_proba)\n",
    "\n",
    "pd.Series(dt.feature_importances_, index=X.columns).sort_values().plot(kind='bar');\n",
    "\n",
    "\n",
    "### Permutation Importance\n",
    "\n",
    "nn = MLPClassifier((50, 50), random_state=1)\n",
    "\n",
    "nn.fit(X_train, y_train)\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "perm = PermutationImportance(nn, random_state=1)\n",
    "\n",
    "perm.fit(X_train, y_train)\n",
    "\n",
    "eli5.show_weights(perm, feature_names=X_train.columns.tolist())\n",
    "\n",
    "\n",
    "### Lime analyzes a single case\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "#### Pick a passenger we want ta analyze\n",
    "passenger_id = 758\n",
    "\n",
    "#### Get the explanation for the 'black box' model\n",
    "exp = explainer.explain_instance(X_test.loc[passenger_id], nn.predict_proba)\n",
    "\n",
    "#### Display explanation\n",
    "exp.show_in_notebook()\n",
    "\n",
    "### Shap\n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "explainer = shap.KernelExplainer(nn.predict_proba, X_train, link='logit')\n",
    "shap_values = explainer.shap_values(X_test, nsamples=100)\n",
    "\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0][0, :], X_test.iloc[0, :], link='logit')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
